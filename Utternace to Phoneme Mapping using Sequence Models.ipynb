{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykh8vQc0p71n"
   },
   "source": [
    "# 11785 HW3P2: Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq0m4w_KkMeQ"
   },
   "source": [
    "**Welcome to HW3P2. In this homework, you will be using the same data from HW1 but will be incorporating sequence models. We recommend you get familaried with sequential data and the working of RNNs, LSTMs and GRUs to have a smooth learning in this part of the homework.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEEll1kGkhcR"
   },
   "source": [
    "Disclaimer: This starter notebook will not be as elaborate as that of HW1P2 or HW2P2. You will need to do most of the implementation in this notebook because, it is expected after 2 HWs, you will be in a position to write a notebook from scratch. You are welcomed to reuse the code from the previous starter notebooks but may also need to make appropriate changes for this homework. <br>\n",
    "We have also given you 3 log files for the Very Low Cutoff (Levenshtein Distance = 30) so that you can observe how loss decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHGaJ_8tx_5Z"
   },
   "source": [
    "Common errors which you may face\n",
    "\n",
    "\n",
    "*   Shape errors: Half of the errors from this homework will account to this category. Try printing the shapes between intermediate steps to debug\n",
    "*   CUDA out of Memory: When your architecture has a lot of parameters, this can happen. Golden keys for this is, (1) Reducing batch_size (2) Call *torch.cuda.empty_cache* often, even inside your training loop, (3) Call *gc.collect* if it helps and (4) Restart run time if nothing works\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_fwJWcpqJDR"
   },
   "source": [
    "# Prelimilaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leymyQ-apwT6"
   },
   "source": [
    "You will need to install packages for decoding and calculating the Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCQtZtkaTrcn",
    "outputId": "34935838-4c6b-4347-be57-c7de66fda199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |██████▌                         | 10 kB 30.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 20 kB 38.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 30 kB 21.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 40 kB 9.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 50 kB 4.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149857 sha256=20c08bc93327c04719051254cdbd24a1dd43d0a78d444afc8c063b24da071046\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "Cloning into 'ctcdecode'...\n",
      "remote: Enumerating objects: 1102, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 1102 (delta 16), reused 28 (delta 13), pack-reused 1063\u001b[K\n",
      "Receiving objects: 100% (1102/1102), 780.91 KiB | 6.62 MiB/s, done.\n",
      "Resolving deltas: 100% (529/529), done.\n",
      "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
      "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
      "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
      "remote: Enumerating objects: 82, done.        \n",
      "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
      "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
      "remote: Enumerating objects: 14073, done.        \n",
      "remote: Counting objects: 100% (386/386), done.        \n",
      "remote: Compressing objects: 100% (312/312), done.        \n",
      "remote: Total 14073 (delta 117), reused 134 (delta 60), pack-reused 13687        \n",
      "Receiving objects: 100% (14073/14073), 5.82 MiB | 16.51 MiB/s, done.\n",
      "Resolving deltas: 100% (7997/7997), done.\n",
      "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
      "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=14dea03018784090956a03c4902f40fcb623189e9a6f081e880ca91e7ad98c75\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "/content/ctcdecode\n",
      "Processing /content/ctcdecode\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Building wheels for collected packages: ctcdecode\n",
      "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13309019 sha256=db7f97106783644303fc6bb3adc4392c65e9a15c91b71e3656cbadc678b5c91b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ajuv7tr0/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
      "Successfully built ctcdecode\n",
      "Installing collected packages: ctcdecode\n",
      "Successfully installed ctcdecode-1.0.3\n",
      "/content\n",
      "Collecting torchsummaryX\n",
      "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.10.0+cu111)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (4.1.1)\n",
      "Installing collected packages: torchsummaryX\n",
      "Successfully installed torchsummaryX-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd ..\n",
    "\n",
    "!pip install torchsummaryX # We also install a summary package to check our model's forward before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4vZbDmJvMp1"
   },
   "source": [
    "**bold text**# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qI4qfx7tiBZt",
    "outputId": "f5deef95-f103-4c5d-ace5-da6308bb9895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as tud\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# imports for decoding and distance calculation\n",
    "import ctcdecode\n",
    "import Levenshtein\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIi0Big7vPa9"
   },
   "source": [
    "# Kaggle (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6gTI0Rslxrr"
   },
   "source": [
    "You need to set up your Kaggle and download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPBUd7Cnl-Rx",
    "outputId": "fa944492-bcbd-4793-cfa8-43089414f5cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle==1.5.8\n",
      "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█████▌                          | 10 kB 34.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 20 kB 22.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 30 kB 16.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 40 kB 15.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 51 kB 8.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 59 kB 4.2 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73275 sha256=ce07ee8070d2daf3ce334a88ab3934c636bcc39f86a0c09614530021078c8113\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"mangalamsahai\",\"key\":\"521f66540469b3a12f7b11566d8b1c14\"}') # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if2Somqfbje1",
    "outputId": "f464738c-f912-4ad2-d56f-f241154e1e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 11-785-s22-hw3p2.zip to /content\n",
      " 99% 1.83G/1.84G [00:30<00:00, 100MB/s] \n",
      "100% 1.84G/1.84G [00:30<00:00, 64.3MB/s]\n",
      "11-785-s22-hw3p2.zip  ctcdecode  hw3p2_student_data  phonemes.py  sample_data\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c 11-785-s22-hw3p2\n",
    "\n",
    "!unzip -q 11-785-s22-hw3p2.zip\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUCKqm1ST1sU"
   },
   "source": [
    "# Dataset and dataloading (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-PjVxPCBvVR6"
   },
   "outputs": [],
   "source": [
    "# PHONEME_MAP is the list that maps the phoneme to a single character. \n",
    "# The dataset contains a list of phonemes but you need to map them to their corresponding characters to calculate the Levenshtein Distance\n",
    "# You final submission should not have the phonemes but the mapped string\n",
    "# No TODOs in this cell\n",
    "\n",
    "PHONEME_MAP = [\n",
    "    \" \",\n",
    "    \".\", #SIL\n",
    "    \"a\", #AA\n",
    "    \"A\", #AE\n",
    "    \"h\", #AH\n",
    "    \"o\", #AO\n",
    "    \"w\", #AW\n",
    "    \"y\", #AY\n",
    "    \"b\", #B\n",
    "    \"c\", #CH\n",
    "    \"d\", #D\n",
    "    \"D\", #DH\n",
    "    \"e\", #EH\n",
    "    \"r\", #ER\n",
    "    \"E\", #EY\n",
    "    \"f\", #F\n",
    "    \"g\", #G\n",
    "    \"H\", #H\n",
    "    \"i\", #IH \n",
    "    \"I\", #IY\n",
    "    \"j\", #JH\n",
    "    \"k\", #K\n",
    "    \"l\", #L\n",
    "    \"m\", #M\n",
    "    \"n\", #N\n",
    "    \"N\", #NG\n",
    "    \"O\", #OW\n",
    "    \"Y\", #OY\n",
    "    \"p\", #P \n",
    "    \"R\", #R\n",
    "    \"s\", #S\n",
    "    \"S\", #SH\n",
    "    \"t\", #T\n",
    "    \"T\", #TH\n",
    "    \"u\", #UH\n",
    "    \"U\", #UW\n",
    "    \"v\", #V\n",
    "    \"W\", #W\n",
    "    \"?\", #Y\n",
    "    \"z\", #Z\n",
    "    \"Z\" #ZH\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "8SndiVRVqBMa"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "from phonemes import PHONEMES\n",
    "# This cell is where your actual TODOs start\n",
    "# You will need to implement the Dataset class by your own. You may also implement it similar to HW1P2 (dont require context)\n",
    "# The steps for implementation given below are how we have implemented it.\n",
    "# However, you are welcomed to do it your own way if it is more comfortable or efficient. \n",
    "\n",
    "class LibriSamples(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
    "\n",
    "        if partition=='train':\n",
    "           self.X_dir = os.path.join(data_path,\"mfcc\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "           self.Y_dir = os.path.join(data_path,\"transcript\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "        elif partition=='dev':\n",
    "           self.X_dir = os.path.join(data_path,\"mfcc\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "           self.Y_dir = os.path.join(data_path,\"transcript\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "           \n",
    "\n",
    "        \n",
    "        self.X_files = os.listdir(self.X_dir)\n",
    "        self.Y_files = os.listdir(self.Y_dir)\n",
    "        \n",
    "        #print(self.X_files.shape)\n",
    "        #self.X=np.zeros(len(self.X_files))\n",
    "        #self.Y=np.zeros(len(self.Y_files))\n",
    "        #for i in range(len(self.X_files)):\n",
    "         #   self.X[i],self.Y[i] =self._getitem_(i)\n",
    "             \n",
    "\n",
    "\n",
    "        #self.Y_files = self.Y1_files[1:-1]\n",
    "        # remove <eos> & <sos> from Y\n",
    "        # TODO: store PHONEMES from phonemes.py inside the class. phonemes.py will be downloaded from kaggle.\n",
    "        # You may wish to store PHONEMES as a class attribute or a global variable as well.\n",
    "        self.PHONEMES = PHONEMES\n",
    "        print(\"X_files\",len(self.X_files))\n",
    "        print(\"Y_files\",len(self.Y_files))\n",
    "        assert(len(self.X_files) == len(self.Y_files))\n",
    "\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_files)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "    \n",
    "        X = np.load(os.path.join(self.X_dir,self.X_files[ind]))    # TODO: Load the mfcc npy file at the specified index ind in the directory\n",
    "        #pdb.set_trace()\n",
    "        Y1 = np.load(os.path.join(self.Y_dir,self.Y_files[ind]))    # TODO: Load the corresponding transcripts\n",
    "        \n",
    "        Y2 =np.array(Y1[1:-1])\n",
    "        #print(Y2)\n",
    "        Y = np.zeros(Y2.shape[0])\n",
    "        \n",
    "        for i in range(Y2.shape[0]):\n",
    "            #pdb.set_trace()\n",
    "            Y[i] = self.PHONEMES.index(Y2[i])\n",
    "            \n",
    "\n",
    "        # Remember, the transcripts are a sequence of phonemes. Eg. np.array(['<sos>', 'B', 'IH', 'K', 'SH', 'AA', '<eos>'])\n",
    "        # You need to convert these into a sequence of Long tensors\n",
    "\n",
    "        # Tip: You may need to use self.PHONEMES\n",
    "        # Remember, PHONEMES or PHONEME_MAP do not have '<sos>' or '<eos>' but the transcripts have them. \n",
    "        # You need to remove '<sos>' and '<eos>' from the trancripts. \n",
    "        # Inefficient way is to use a for loop for this. Efficient way is to think that '<sos>' occurs at the start and '<eos>' occurs at the end.\n",
    "        \n",
    "        # Amend the self.Phonemes list\n",
    "        \n",
    "        Yy = torch.LongTensor(Y)      # TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
    "        #pdb.set_trace()\n",
    "        return torch.tensor(X), Yy\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "\n",
    "        #batch_x=[]\n",
    "        #batch_y=[]\n",
    "        #batch_x_pad=[]\n",
    "        #batch_y_pad=[]\n",
    "        #lengths_x=[]\n",
    "        #lengths_y=[]\n",
    "        batch_x = [x for x,y in batch]\n",
    "        #pdb.set_trace()\n",
    "        batch_y = [y for x,y in batch]\n",
    "        #for i in range(len(batch)):\n",
    "        #    batch_x.append(torch.from_numpy(np.array([x for x,y in batch])[i]))\n",
    "        #    batch_y.append(np.array([y for x,y in batch])[i])\n",
    "        \n",
    "                         # TODO: pad the sequence with pad_sequence (already imported)\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        batch_x_pad = pad_sequence(batch_x)\n",
    "        #pdb.set_trace()\n",
    "        lengths_x = [x.shape[0] for x,y in batch]                   # TODO: Get original lengths of the sequence before padding\n",
    "        batch_y_pad = pad_sequence(batch_y,batch_first=True)        # TODO: pad the sequence with pad_sequence (already imported)\n",
    "        lengths_y = [y.shape[0] for x,y in batch]                   # TODO: Get original lengths of the sequence before padding\n",
    "        #pdb.set_trace()\n",
    "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
    "\n",
    "\n",
    "# You can either try to combine test data in the previous class or write a new Dataset class for test data\n",
    "class LibriSamplesTest(torch.utils.data.Dataset):\n",
    "\n",
    "     def __init__(self, data_path, test_order): # test_order is the csv similar to what you used in hw1\n",
    "         self.data_path=data_path\n",
    "         \n",
    "         test_order_list = list(pd.read_csv(data_path + test_order).file)\n",
    "         self.X = [np.load(data_path + \"/mfcc/\"+v) for v in test_order_list]\n",
    "#          # self.X_dir = os.path.join(data_path,\"mfcc\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "#            #self.Y_dir = os.path.join(data_path,\"transcript\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "        \n",
    "#         # elif partition=='dev':\n",
    "#         #    self.X_dir = os.path.join(data_path,\"mfcc\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "#         #    self.Y_dir = os.path.join(data_path,\"transcript\") # data_path = /content/hw3p2_student_data/hw3p2_student_data\n",
    "#         # self.X_files = os.listdir(self.X_dir)\n",
    "          # if test_order:\n",
    "#         #    with open(test_order) as f:  # Need for csv file here.\n",
    "#         #         subset = csv.reader(f) # read csv file\n",
    "#         #         test_order_list = list(subset) # TODO: open test_order.csv as a list\n",
    "#         #    for file in test_order_list:  \n",
    "#         #        self.X.append(np.load(os.path.join(\"/content/hw3p2_student_data/hw3p2_student_data/test\",file)))     \n",
    "\n",
    "#         # Load .npy files.\n",
    "                \n",
    "#         else:\n",
    "       \n",
    "#         # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
    "    \n",
    "     def __len__(self):\n",
    "         return len(self.X)\n",
    "    \n",
    "     def __getitem__(self, ind):\n",
    "         # TODOs: Need to return only X because this is the test dataset\n",
    "\n",
    "         return torch.tensor(self.X[ind])\n",
    "    \n",
    "     def collate_fn(batch):\n",
    "        # batch_x=[]\n",
    "        # batch_x_pad=[]\n",
    "        # lengths_x=[]\n",
    "        # for i in range(len(batch)):\n",
    "        #     batch_x.append(torch.from_numpy(np.array([x for x in batch])[i]))\n",
    "            \n",
    "        \n",
    "        #                  # TODO: pad the sequence with pad_sequence (already imported)\n",
    "        \n",
    "        # batch_x_pad.append(pad_sequence(batch_x))\n",
    "        # lengths_x.append(len(batch_x))                   # TODO: Get original lengths of the sequence before padding\n",
    "        #         # TODO: pad the sequence with pad_sequence (already imported)\n",
    "         \n",
    "         \n",
    "         \n",
    "        #      # TODO: pad the sequence with pad_sequence (already imported)\n",
    "        #       # TODO: Get original lengths of the sequence before padding\n",
    "        batch_x = [x for x in batch]\n",
    "        #pdb.set_trace()\n",
    "        #batch_y = [y for x,y in batch]\n",
    "        #for i in range(len(batch)):\n",
    "        #    batch_x.append(torch.from_numpy(np.array([x for x,y in batch])[i]))\n",
    "        #    batch_y.append(np.array([y for x,y in batch])[i])\n",
    "        \n",
    "                         # TODO: pad the sequence with pad_sequence (already imported)\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        batch_x_pad = pad_sequence(batch_x)\n",
    "        lengths_x = [x.shape[0] for x in batch]                   # TODO: Get original lengths of the sequence before padding\n",
    "        \n",
    "        return batch_x_pad, torch.tensor(lengths_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mzoYfTKu14s",
    "outputId": "1d79dd28-65b7-4533-a60b-bd9d09d36ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_files 28539\n",
      "Y_files 28539\n",
      "X_files 2703\n",
      "Y_files 2703\n",
      "Batch size:  64\n",
      "Train dataset samples = 28539, batches = 446\n",
      "Val dataset samples = 2703, batches = 43\n",
      "Test dataset samples = 2620, batches = 41\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "root = \"/content/hw3p2_student_data/hw3p2_student_data\" # TODO: Where your hw3p2_student_data folder is\n",
    "\n",
    "train_data = LibriSamples(os.path.join(root,\"train\"),partition='train')\n",
    "val_data = LibriSamples(os.path.join(root,'dev'),partition='dev')\n",
    "test_data = LibriSamplesTest(os.path.join(root,'test'),'/test_order.csv')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,collate_fn=LibriSamples.collate_fn) # TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
    "val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=True,collate_fn=LibriSamples.collate_fn) # TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False,collate_fn=LibriSamplesTest.collate_fn) # TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
    "\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "XYBHRyN7tEVl"
   },
   "outputs": [],
   "source": [
    "x,y,lx,ly = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9FwVZ9I2da0",
    "outputId": "cc69e6f9-567e-4b5a-e418-b7f58f83c944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2436, 64, 13]) torch.Size([64, 288]) torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Optional\n",
    "# Test code for checking shapes and return arguments of the train and val loaders\n",
    "for data in val_loader:\n",
    "    x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h25bIAah8Ib8",
    "outputId": "65079e7c-b881-41ab-ca00-963ee42c671f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(lx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly4mjUUUuJhy"
   },
   "source": [
    "# Model Configuration (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "RxirnS6K9GKj"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "       nn.init.kaiming_normal(m.weight)\n",
    "       nn.init.normal_(m.bias)\n",
    "\n",
    "    elif isinstance(m,nn.Conv1d):\n",
    "         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    elif isinstance(m,nn.BatchNorm1d):\n",
    "         nn.init.constant_(m.weight,1)\n",
    "         nn.init.constant_(m.bias,0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "lvFhHgQiBdtY"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=7, padding=3, groups=dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.Conv1d(dim, dim * 4, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(dim * 4, dim, kernel_size=1)\n",
    "        )\n",
    "        self.layer_scale = nn.Parameter(torch.ones(1, dim, 1) * 1e-2, requires_grad=True)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        x = self.backbone(x)\n",
    "        x = self.layer_scale * x\n",
    "        x += out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGoiXd70tb5z",
    "outputId": "263243b5-17df-4fea-d735-18c3f1ec7a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (embedding): Sequential(\n",
      "    (0): Conv1d(13, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU()\n",
      "    (3): Block(\n",
      "      (backbone): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), groups=256)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "        (3): GELU()\n",
      "        (4): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, num_layers=4, dropout=0.4, bidirectional=True)\n",
      "  (classification): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (1): GELU()\n",
      "    (2): Linear(in_features=1024, out_features=41, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "from torch.nn.modules import dropout\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size=13,hidden_size=256): # You can add any extra arguments as you wish\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
    "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
    "        self.embedding = nn.Sequential(nn.Conv1d(in_channels=input_size,out_channels=hidden_size,kernel_size=3,stride=2,padding=1,bias=False),\n",
    "                                       nn.BatchNorm1d(hidden_size),\n",
    "                                       #nn.Conv1d(128,256,kernel_size=1,stride=1)\n",
    "                                       nn.GELU(),\n",
    "                                       Block(hidden_size),\n",
    "                                       nn.Dropout(0.4)\n",
    "                                       )\n",
    "\n",
    "        # nn.Sequential(nn.Conv1d(13,64,kernel_size=3,stride=1,bias=False,padding=1),\n",
    "        #                                nn.BatchNorm1d(64),\n",
    "        #                                nn.LeakyReLU(),\n",
    "        #                                #nn.MaxPool1d(kernel_size=2,stride=2),\n",
    "        #                                #nn.Dropout(p=0.45),\n",
    "        #                                nn.Conv1d(64,128,stride=1,kernel_size=1),\n",
    "        #                                nn.BatchNorm1d(128),\n",
    "        #                                nn.LeakyReLU(),\n",
    "        #                                #nn.MaxPool1d(kernel_size=2,stride=2),\n",
    "        #                                nn.Dropout(0.45),\n",
    "        #                                #nn.Conv1d(256,512,stride=1,kernel_size=1),\n",
    "        #                                #nn.BatchNorm1d(512),\n",
    "        #                                #nn.LeakyReLU(),\n",
    "        #                                #nn.MaxPool1d(kernel_size=2,stride=2),\n",
    "        #                                #nn.Dropout(0.45),\n",
    "        #                               )\n",
    "            \n",
    "        # Frequency Masking.\n",
    "        # GPU With High RAM\n",
    "        # for p in self.embedding:\n",
    "        #     if isinstance(p,nn.Conv1d):\n",
    "        #        nn.init.kaiming_normal_(p.weight,mode='fan_out',nonlinearity='relu')\n",
    "        #     elif isinstance(p,nn.BatchNorm1d):\n",
    "        #          nn.init.constant_(p.weight,1)\n",
    "        #          nn.init.constant_(p.bias,0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,num_layers=4,bidirectional=True,dropout=0.4)\n",
    "\n",
    "\n",
    "        #self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=3)# TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
    "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "                \n",
    "        self.classification = nn.Sequential(nn.Linear(hidden_size*2,hidden_size*4),\n",
    "                                            nn.GELU(),\n",
    "                                            #nn.Dropout(p=0.45),\n",
    "                                            nn.Linear(hidden_size*4,41)\n",
    "                                            )\n",
    "        #nn.Linear(hidden_size,41)# TODO: Create a single classification layer using nn.Linear()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, l): # TODO: You need to pass atleast 1 more parameter apart from self and x\n",
    "\n",
    "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        out = torch.permute(x,(1,2,0))\n",
    "        out = self.embedding(out)\n",
    "        out = torch.permute(out,(2,0,1))\n",
    "        #pdb.set_trace()\n",
    "        #out = out.permute(2,0,1)\n",
    "        #pdb.set_trace()\n",
    "        #l = torch.div(l,8)\n",
    "        #l = l.clamp(max=out.shape[2])\n",
    "        l = ((l - 3 + 2) // 2) + 1\n",
    "        packed_input = pack_padded_sequence(out,l,enforce_sorted=False) # TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
    "        #pdb.set_trace()\n",
    "        out1, (out2, out3) = self.lstm(packed_input)# TODO: Pass packed input to self.lstm\n",
    "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
    "        out, lengths  = pad_packed_sequence(out1) # TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
    "        #pdb.set_trace()\n",
    "        out = self.classification(out)# TODO: Pass unpacked LSTM output to the classification layer\n",
    "        out = out.log_softmax(2)# Optional: Do log softmax on the output. Which dimension?\n",
    "        #pdb.set_trace()\n",
    "        return out,lengths # TODO: Need to return 2 variables\n",
    "\n",
    "model = Network().to(device)\n",
    "print(model)\n",
    "#summary(model, x.to(device), lx) # x and lx are from the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkEv7Z5bET3-"
   },
   "source": [
    "## CovNext + Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "YMBRYLOwyiaX"
   },
   "outputs": [],
   "source": [
    "class StageLayers(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 stride,\n",
    "               padding,\n",
    "               flag,\n",
    "                ):\n",
    "        super().__init__() # Just have to do this for all nn.Module classes\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.padding= padding\n",
    "        self.flag= flag\n",
    "        #self.drop_block = DropBlock2D(block_size=3, drop_prob=0.3)        \n",
    "        self.Layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels,out_channels=out_channels,kernel_size=7,stride=self.stride,padding=self.padding,groups=in_channels),\n",
    "            nn.BatchNorm1d(num_features=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=out_channels,out_channels=4*out_channels,kernel_size=1,stride=1,padding=0),\n",
    "            nn.BatchNorm1d(num_features=4*out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=4*out_channels,out_channels=out_channels,kernel_size=1,stride=1,padding=0),\n",
    "            nn.BatchNorm1d(num_features=out_channels),\n",
    "            #nn.Conv2d(in_channels=in_channel,out_channels=in_channel*4,kernel_size=1,stride=1,padding=0),\n",
    "         )\n",
    "        \n",
    "  def forward(self, x):\n",
    "        \n",
    "        #y= torch.tensor(nn.Identity(x))\n",
    "        size = (x.shape[2]+2*self.padding-7)//self.stride +1\n",
    "        y= nn.functional.interpolate(x,size)\n",
    "        out = self.Layer(x)\n",
    "\n",
    "        if self.flag==1:\n",
    "          #  print(\"out,x\",x.shape)\n",
    "          #  print(\"out,y\",y.shape)\n",
    "           return out\n",
    "        \n",
    "        #if self.in_channels==self.out_channels and x.shape[2]==out.shape[2]:\n",
    "             #print(out.shape)\n",
    "        #      #print(x.shape)\n",
    "        # elif self.flag==2:\n",
    "        #    return out\n",
    "        else: \n",
    "           #pdb.set_trace()\n",
    "          #  print(\"out+y,y.shape\",y.shape)\n",
    "          #  print(\"out+y,out.shape\",out.shape)\n",
    "          #  print(\"out+y,flag\",self.flag)\n",
    "           return out+y\n",
    "      \n",
    "    \n",
    "        \n",
    "class CovNext(nn.Module):\n",
    "     def __init__(self, num_classes= 256):\n",
    "         super().__init__() # Already features indented from previous class. \n",
    "         #self.drop_block = DropBlock2D(block_size=3, drop_prob=0.3)  \n",
    "         self.num_classes = num_classes\n",
    "         \n",
    "         self.stem = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=13,out_channels=96,kernel_size=4,stride=4,padding=0),\n",
    "            nn.BatchNorm1d(num_features=96),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "         \n",
    "        #  self.stage1_maxpool = nn.Sequential(\n",
    "        #    nn.MaxPool2d(kernel_size=3,stride=2,padding=1), \n",
    "        #     )\n",
    "        # # 56*56\n",
    "         self.stage_cfgs = [\n",
    "              # in_channel, #blocks\n",
    "            [96, 96, 3],\n",
    "            [96, 192, 3], \n",
    "            [192, 384, 9], \n",
    "            [384, 768, 3], \n",
    "            ]\n",
    "                           \n",
    "         layers = []\n",
    "         #pdb.set_trace()\n",
    "         i=0\n",
    "         for curr_stage in self.stage_cfgs:\n",
    "             in_channels, out_channels, num_blocks = curr_stage\n",
    "             for block_idx in range(num_blocks):\n",
    "                 print(i)\n",
    "                 print(block_idx)\n",
    "                 if block_idx==0: \n",
    "                       stride=1 if in_channels==13 else 2\n",
    "                       padding=0 if in_channels==13 else 3\n",
    "                      #  flag=2\n",
    "                 else:\n",
    "                    stride= 1\n",
    "                    padding= 3\n",
    "                    #flag=0      \n",
    "\n",
    "                #  if block_idx==num_blocks-1:\n",
    "                #     flag=1\n",
    "                #  else:\n",
    "                #      flag=0     \n",
    "\n",
    "                 layers.append(StageLayers(\n",
    "                 in_channels = in_channels,\n",
    "                 out_channels = out_channels,\n",
    "                 padding = padding,  \n",
    "                 stride= stride,\n",
    "                 flag= 1 if block_idx==0 else 0\n",
    "                 ))\n",
    "                 in_channels = out_channels\n",
    "             \n",
    "             i=i+1  \n",
    "                 \n",
    "                # in_channels=num_channels\n",
    "                # In channels of the next block is the out_channels of the current one\n",
    "                # in_channels = in_channels*4 \n",
    "            \n",
    "         self.layers = nn.Sequential(*layers) # Done, save them to the class\n",
    "\n",
    "       \n",
    "         self.cls_layer = nn.Sequential(\n",
    "             #nn.Dropout(p=0.1),\n",
    "             #pdb.set_trace(),\n",
    "             nn.AdaptiveAvgPool1d(256),\n",
    "            #  nn.Flatten(),\n",
    "            #  nn.Linear(768,num_classes),\n",
    "           )\n",
    "\n",
    "      #   self._initialize_weights()\n",
    "\n",
    "     #def _initialize_weights(self):\n",
    "     #   \"\"\"\n",
    "     #   Usually, I like to use default pytorch initialization for stuff, but\n",
    "     #   MobileNetV2 made a point of putting in some custom ones, so let's just\n",
    "     #   use them.\n",
    "     #   \"\"\"\n",
    "     #   for m in self.modules():\n",
    "     #       if isinstance(m, nn.Conv2d):\n",
    "     #          n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "     #           m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "     #           if m.bias is not None:\n",
    "     #              m.bias.data.zero_()\n",
    "     #       elif isinstance(m, nn.BatchNorm2d):\n",
    "     #           m.weight.data.fill_(1)\n",
    "     #          m.bias.data.zero_()\n",
    "     #       elif isinstance(m, nn.Linear):\n",
    "     #           m.weight.data.normal_(0, 0.01)\n",
    "     #          m.bias.data.zero_()\n",
    "\n",
    "     def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        #out = self.drop_block(out)\n",
    "        #out = self.stage1_maxpool(out)\n",
    "        #pdb.set_trace()\n",
    "        out = self.layers(out)\n",
    "        #print(out.shape)\n",
    "        #pdb.set_trace()\n",
    "        out = self.cls_layer(out)\n",
    "        \n",
    "        \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJjuwBu7fD1f"
   },
   "source": [
    "# New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYwuWq9EfBDm"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=3, padding=1, groups=dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.Conv1d(dim, dim * 4, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(dim * 4, dim, kernel_size=1)\n",
    "        )\n",
    "        self.layer_scale = nn.Parameter(torch.ones(1, dim, 1) * 1e-2, requires_grad=True)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        x = self.backbone(x)\n",
    "        x = self.layer_scale * x\n",
    "        x += out\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwunYpyugFg"
   },
   "source": [
    "# Training Configuration (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "iGoozH2nd6KB"
   },
   "outputs": [],
   "source": [
    "epoches= 200\n",
    "lr=2e-3\n",
    "criterion = nn.CTCLoss() # TODO: What loss do you need for sequence to sequence models? \n",
    "# Do you need to transpose or permute the model output to find out the loss? Read its documentation\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5) # TODO: Adam works well with LSTM (use lr = 2e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, min_lr=1e-7)\n",
    "decoder = CTCBeamDecoder(\n",
    "    labels=PHONEMES,\n",
    "    model_path=None,\n",
    "    alpha=0,\n",
    "    beta=0,\n",
    "    beam_width=10,\n",
    "    log_probs_input=True\n",
    ")\n",
    "# TODO: Intialize the CTC beam decoder\n",
    "# Check out https://github.com/parlance/ctcdecode for the details on how to implement decoding\n",
    "# Do you need to give log_probs_input = True or False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "KEuvs3Kje47-"
   },
   "outputs": [],
   "source": [
    "# this function calculates the Levenshtein distance \n",
    "\n",
    "def calculate_levenshtein(h, y, lh, ly, decoder, PHONEME_MAP):\n",
    "\n",
    "    # h - ouput from the model. Probability distributions at each time step \n",
    "    # y - target output sequence - sequence of Long tensors\n",
    "    # lh, ly - Lengths of output and target\n",
    "    # decoder - decoder object which was initialized in the previous cell\n",
    "    # PHONEME_MAP - maps output to a character to find the Levenshtein distance\n",
    "   \n",
    "    # TODO: You may need to transpose or permute h based on how you passed it to the criterion\n",
    "    beam_results, beam_scores, timesteps, out_seq_len =decoder.decode(torch.permute(h,(1,0,2)),seq_lens=lh)\n",
    "    # Print out the shapes often to debug\n",
    "     \n",
    "    # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
    "    # Input to the decode method will be h and its lengths lh \n",
    "    # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
    "\n",
    "    batch_size = h.size()[1]\n",
    "\n",
    "\n",
    "    dist = 0\n",
    "\n",
    "    for i in range(batch_size): # Loop through each element in the batch\n",
    "\n",
    "        h_sliced = beam_results[i,0,0:out_seq_len[i][0]]# TODO: Get the output as a sequence of numbers from beam_results\n",
    "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
    "        # Same goes for beam_results and out_lens\n",
    "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
    "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
    "\n",
    "        h_string = ''.join([PHONEME_MAP[hi]for hi in h_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
    "\n",
    "        y_sliced = y[i][:ly[i].item()]# TODO: Do the same for y - slice off the padding with ly\n",
    "        y_string = ''.join([PHONEME_MAP[yi] for yi in y_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
    "        \n",
    "        dist += Levenshtein.distance(h_string, y_string)\n",
    "\n",
    "    dist/=batch_size\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8CSq8Nw5gt2"
   },
   "outputs": [],
   "source": [
    "m = nn.AdaptiveAvgPool1d(5)\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "d7b7iY0we8Kj"
   },
   "outputs": [],
   "source": [
    "# Optional but recommended\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    \n",
    "    # Write a test code do perform a single forward pass and also compute the Levenshtein distance\n",
    "    # Make sure that you are able to get this right before going on to the actual training\n",
    "    # You may encounter a lot of shape errors\n",
    "    # Printing out the shapes will help in debugging\n",
    "    # Keep in mind that the Loss which you will use requires the input to be in a different format and the decoder expects it in a different format\n",
    "    # Make sure to read the corresponding docs about it\n",
    "    x,y,lx,ly = data\n",
    "    x = x.to(device) #cuda()\n",
    "    y = y.to(device) #cuda()\n",
    "    # # lx = lx.cuda()\n",
    "    #lx = lx.to(device)\n",
    "    #pdb.set_trace()\n",
    "    outputs,l = model(x,lx)\n",
    "\n",
    "    break # one iteration is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "Q5npQNFH315V"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # Use this often\n",
    "\n",
    "# TODO: Write the model evaluation function if you want to validate after every epoch\n",
    "def model_eval():\n",
    "  total_d = 0\n",
    "  total_l = 0\n",
    "  val_loss = 0\n",
    "  for i,data in enumerate(val_loader):\n",
    "      x,y,lx,ly = data\n",
    "    \n",
    "      x= x.cuda()\n",
    "      y= y.cuda()\n",
    "\n",
    "      with torch.no_grad():\n",
    "           outputs, l =model(x,lx)\n",
    "           loss = criterion(outputs,y,l,ly)\n",
    "      val_loss += loss     \n",
    "    #pdb.set_trace()\n",
    "      d = calculate_levenshtein(outputs,y,l, ly,decoder,PHONEME_MAP) ## (BatchSize, Seqlen, Features=41)\n",
    "\n",
    "      total_d +=float(d)\n",
    "  model.train() \n",
    "  print(f\"loss: {total_loss / len(val_loader)}, Average Distance: {total_d / len(val_loader)}\")\n",
    "    \n",
    "  return  val_loss  \n",
    "\n",
    "# You are free to write your own code for model evaluation or you can use the code from previous homeworks' starter notebooks\n",
    "# However, you will have to make modifications because of the following.\n",
    "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
    "# (2) The model forward returns 2 outputs\n",
    "# (3) The loss may require transpose or permuting\n",
    "\n",
    "# Note that when you give a higher beam width, decoding will take a longer time to get executed\n",
    "# Therefore, it is recommended that you calculate only the val dataset's Levenshtein distance (train not recommended) with a small beam width\n",
    "# When you are evaluating on your test set, you may have a higher beam width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNbi0ZpqpNjg",
    "outputId": "dc65ece5-66bb-440c-93e0-11fca5ad68d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22BOsw2__VgO"
   },
   "outputs": [],
   "source": [
    "save_checkpoint = torch.load(\"/content/drive/MyDrive/BiDirLSTM_LRScheduler_model_epoch_27.pth\") # Epoch done till now 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6lPezXk_He_",
    "outputId": "22de285e-73a5-4883-819c-02ff943b631e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(save_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPpGWhli_c-m"
   },
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(save_checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssCOUVLRABqK"
   },
   "outputs": [],
   "source": [
    "epoch = save_checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "MG4F77Nm0Am9",
    "outputId": "86372fbe-fb94-4aac-b361-92addf149895"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: Train Loss 2.4286, Learning Rate 0.0020 0.0\n",
      "loss: 25.18957822128784, Average Distance: 29.192296511627905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: Train Loss 0.7553, Learning Rate 0.0020 0.0\n",
      "loss: 7.834122297375701, Average Distance: 20.38825096899225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: Train Loss 0.5558, Learning Rate 0.0020 0.0\n",
      "loss: 5.764792332122492, Average Distance: 16.999006782945738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: Train Loss 0.4674, Learning Rate 0.0020 0.0\n",
      "loss: 4.8480757488760835, Average Distance: 15.251501937984495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200: Train Loss 0.4155, Learning Rate 0.0020 0.0\n",
      "loss: 4.309796738070111, Average Distance: 15.622141472868217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200: Train Loss 0.3780, Learning Rate 0.0020 0.0\n",
      "loss: 3.9206904165966567, Average Distance: 14.709883720930232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: Train Loss 0.3511, Learning Rate 0.0020 0.0\n",
      "loss: 3.6413888252058695, Average Distance: 13.221487403100776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200: Train Loss 0.3346, Learning Rate 0.0020 0.0\n",
      "loss: 3.4706220155538516, Average Distance: 12.86419573643411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200: Train Loss 0.3173, Learning Rate 0.0020 0.0\n",
      "loss: 3.291118153999018, Average Distance: 12.6765503875969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200: Train Loss 0.3046, Learning Rate 0.0020 0.0\n",
      "loss: 3.1595571609430535, Average Distance: 12.246027131782945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   2%|▏         | 10/446 [00:10<06:58,  1.04it/s, loss=0.2794, lr=0.0020]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-251767481d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "torch.cuda.empty_cache()\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epoches)) # New Addition\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=3, mode='min', verbose=True)\n",
    "# TODO: Write the model training code \n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# You are free to write your own code for training or you can use the code from previous homeworks' starter notebooks\n",
    "model.train()\n",
    "# However, you will have to make modifications because of the following.\n",
    "for epoch in range(epoches):\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    total_dist = 0\n",
    "    total_loss = 0\n",
    "    Val_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y, lx, ly = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs, l = model(x, lx)\n",
    "            #print(i)\n",
    "            #pdb.set_trace()\n",
    "            loss = criterion(outputs, y, l, ly)      #.permute(1,0,2)\n",
    "\n",
    "        #num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        #pdb.set_trace()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "                 \n",
    "        batch_bar.update()\n",
    "    #Val_loss = model_eval()\n",
    "    #scheduler.step(Val_loss)   # Validation Loss  \n",
    "    batch_bar.close()\n",
    "    \n",
    "    print(\"Epoch {}/{}: Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "        epoch + 1,\n",
    "        epoches,\n",
    "        float(total_loss / len(train_loader)),\n",
    "        float(optimizer.param_groups[0]['lr'])),\n",
    "        float(float(total_dist / len(train_loader))))\n",
    "    \n",
    "    # if epoch%3==0: \n",
    "    #    model_eval()\n",
    "    Val_loss = model_eval()\n",
    "    scheduler.step(Val_loss)\n",
    "\n",
    "    checkpoint= {\n",
    "               'epoch': epoch,\n",
    "               'model_state_dict': model.state_dict(),\n",
    "               'optimizer_state_dict': optimizer.state_dict(),\n",
    "               'loss': criterion,\n",
    "               }\n",
    "    torch.save(checkpoint,\"/content/drive/MyDrive/Mangalam_model_epoch_\"+str(epoch)+\".pth\")\n",
    "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
    "# (2) The model forward returns 2 outputs\n",
    "# (3) The loss may require transpose or permuting\n",
    "\n",
    "# Tip: Implement mixed precision training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROrqXnNqzJSc"
   },
   "source": [
    "# Submit to kaggle (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQTw13KtOO9R"
   },
   "outputs": [],
   "source": [
    "save_checkpoint = torch.load(\"/content/drive/MyDrive/model_epoch_29.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQWd1NutOgDl",
    "outputId": "5df1c888-19d7-4103-bb33-36604d820d3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(save_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhMRO8ccwDVl"
   },
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(save_checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Htpj7-Q9s00"
   },
   "outputs": [],
   "source": [
    "def model_eval():\n",
    "  total_d=0\n",
    "  for i,data in enumerate(val_loader):\n",
    "    x,y,lx,ly = data\n",
    "    x= x.cuda()\n",
    "    y= y.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs, l =model(x,lx)\n",
    "\n",
    "      d = calculate_levenshtein(outputs,y,l, ly,decoder,PHONEME_MAP)\n",
    "\n",
    "      total_d +=float(d)\n",
    "\n",
    "  print(\"Total Distance:{},Average Distance:{}\",format(total_d),format(total_d/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "zFomxtnt97Kj",
    "outputId": "cb70b2bb-51e0-4c21-dd13-657f16a94739"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-28a7223e19e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-54a6e12b432a>\u001b[0m in \u001b[0;36mmodel_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_levenshtein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPHONEME_MAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mtotal_d\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ad2baf8896e1>\u001b[0m in \u001b[0;36mcalculate_levenshtein\u001b[0;34m(h, y, lh, ly, decoder, PHONEME_MAP)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# TODO: You may need to transpose or permute h based on how you passed it to the criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbeam_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_seq_len\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_lens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Print out the shapes often to debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ctcdecode/__init__.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, probs, seq_lens)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mout_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             )\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-SU9fZ3xHtk"
   },
   "outputs": [],
   "source": [
    "# TODO: Write your model evaluation code for the test dataset\n",
    "# You can write your own code or use from the previous homewoks' stater notebooks\n",
    "# You can't calculate loss here. Why?\n",
    "model.eval()\n",
    "pred = []\n",
    "for i, data in enumerate(test_loader,0):\n",
    "    x, lx = data\n",
    "    x = x.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, l = model(x, lx)\n",
    "\n",
    "    beam_results, beam_scores, timesteps, out_lengths = decoder.decode(torch.permute(outputs, (1, 0, 2)), seq_lens=l)\n",
    "\n",
    "    for i in range(outputs.size()[1]): # Loop through each element in the batch\n",
    "        \n",
    "        h_sliced = beam_results[i, 0, 0:out_lengths[i][0]]\n",
    "        h_string = ''.join([PHONEME_MAP[hi] for hi in h_sliced])\n",
    "        pred.append(h_string)\n",
    "\n",
    "df = pd.DataFrame(pred, columns=['predictions'])\n",
    "df.index.name = 'id'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "RzVqSiZW6xRA",
    "outputId": "63404392-7400-48cd-8b7e-c95906aaf90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d8428f7a-6dc3-4633-8710-2027cec4ccc2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.HwtytOldDhtRUhTDhstrgOkRrst.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.RImEnyimpoRI.DIIvliNhzmOstlhvlI.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.Hwn?UmEbIWhniNaR?UtbIgiN?emhSrpIs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..pHIbhlsofrhnDhmyt.ekses.brnz.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..DesWiTwvRekhniNinDhpEnzhvDhHaRt.hnsOtgOz.on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.DhcyldHAdidnEdhv.gREs.WicdiznatinbARIhblI.kOg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.olhbwtHimWhzhthmhldhvbRyt.hndbROkhnkelrzkAtrd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.EfhtbIgRenjhn.frst.DhtDIt.ots.hvisrdhnkeRktr....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.DhpitIDetmImhstwn.ANdh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.HIWhzshvtHaRdidinipechs.sedbeT.AndbIiNhnlhvHI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8428f7a-6dc3-4633-8710-2027cec4ccc2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d8428f7a-6dc3-4633-8710-2027cec4ccc2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d8428f7a-6dc3-4633-8710-2027cec4ccc2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                          predictions\n",
       "id                                                   \n",
       "0                       .HwtytOldDhtRUhTDhstrgOkRrst.\n",
       "1                   .RImEnyimpoRI.DIIvliNhzmOstlhvlI.\n",
       "2                 .Hwn?UmEbIWhniNaR?UtbIgiN?emhSrpIs.\n",
       "3                     ..pHIbhlsofrhnDhmyt.ekses.brnz.\n",
       "4      ..DesWiTwvRekhniNinDhpEnzhvDhHaRt.hnsOtgOz.on.\n",
       "5   .DhcyldHAdidnEdhv.gREs.WicdiznatinbARIhblI.kOg...\n",
       "6   .olhbwtHimWhzhthmhldhvbRyt.hndbROkhnkelrzkAtrd...\n",
       "7   .EfhtbIgRenjhn.frst.DhtDIt.ots.hvisrdhnkeRktr....\n",
       "8                            .DhpitIDetmImhstwn.ANdh.\n",
       "9   .HIWhzshvtHaRdidinipechs.sedbeT.AndbIiNhnlhvHI..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ-Jk1TB647b"
   },
   "outputs": [],
   "source": [
    "print(\"Train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE1hRnvf0bFz"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate the csv file\n",
    "df.to_csv('/content/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjDr-P_1cbw0",
    "outputId": "3347f863-0931-478f-e338-879c181b57df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
      "100% 204k/204k [00:02<00:00, 92.2kB/s]\n",
      "Successfully submitted to Automatic Speech Recognition (ASR)"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c 11-785-s22-hw3p2 -f submission.csv -m New_Submission"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "HW3P2_Final_6.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
